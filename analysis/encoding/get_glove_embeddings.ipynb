{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "completed-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/pre-trained-word-embedding-using-glove-in-nlp-models/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-accounting",
   "metadata": {},
   "source": [
    "# Glove Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "above-indicator",
   "metadata": {},
   "source": [
    "This notebook gets word embeddings for target words in a given movie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-weapon",
   "metadata": {},
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hourly-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-truck",
   "metadata": {},
   "source": [
    "## Make a dictionary of words in shrek transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "retired-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToNum = {}\n",
    "count = 1\n",
    "\n",
    "with open('shrek_transcript_final.csv', newline='', encoding='utf8') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    \n",
    "    # Skip the first row (header)\n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        word = row[0] \n",
    "        wordToNum[word] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "current-percentage",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'okay' : 151\n",
      "'let' : 2\n",
      "'me' : 319\n",
      "'get' : 247\n",
      "'this' : 5\n",
      "'straight' : 6\n",
      "'you' : 303\n",
      "'want' : 8\n",
      "'to' : 144\n",
      "'go' : 10\n",
      "'fight' : 11\n",
      "'a' : 312\n",
      "'dragon' : 13\n",
      "'and' : 122\n",
      "'rescue' : 15\n",
      "'princess' : 17\n",
      "'just' : 314\n",
      "'so' : 19\n",
      "'farquad' : 20\n",
      "'will' : 21\n",
      "'give' : 22\n",
      "'back' : 24\n",
      "'swamp' : 26\n",
      "'which' : 27\n",
      "'only' : 29\n",
      "'don't' : 255\n",
      "'have' : 304\n",
      "'cuz' : 309\n",
      "'he' : 33\n",
      "'filled' : 34\n",
      "'it' : 196\n",
      "'full' : 36\n",
      "'of' : 275\n",
      "'freaks' : 38\n",
      "'in' : 171\n",
      "'the' : 315\n",
      "'first' : 41\n",
      "'place' : 42\n",
      "'that' : 127\n",
      "'about' : 44\n",
      "'right' : 45\n",
      "'know' : 295\n",
      "'what' : 232\n",
      "'maybe' : 100\n",
      "'there's' : 140\n",
      "'good' : 129\n",
      "'reason' : 53\n",
      "'donkeys' : 54\n",
      "'shouldn't' : 55\n",
      "'talk' : 56\n",
      "'i' : 298\n",
      "'shrek' : 61\n",
      "'why' : 62\n",
      "'pull' : 66\n",
      "'some' : 248\n",
      "'ogre' : 94\n",
      "'stuff' : 71\n",
      "'on' : 289\n",
      "'him' : 77\n",
      "'throttle' : 76\n",
      "'lay' : 78\n",
      "'siege' : 79\n",
      "'his' : 84\n",
      "'fortress' : 82\n",
      "'grind' : 83\n",
      "'bones' : 85\n",
      "'make' : 162\n",
      "'your' : 300\n",
      "'bread' : 89\n",
      "'whole' : 291\n",
      "'trip' : 95\n",
      "'oh' : 201\n",
      "'could' : 102\n",
      "'decapitated' : 104\n",
      "'an' : 105\n",
      "'entire' : 106\n",
      "'village' : 107\n",
      "'put' : 109\n",
      "'their' : 124\n",
      "'heads' : 111\n",
      "'plate' : 114\n",
      "'grab' : 115\n",
      "'knife' : 117\n",
      "'cut' : 118\n",
      "'open' : 119\n",
      "'spleen' : 121\n",
      "'drink' : 123\n",
      "'fluids' : 125\n",
      "'does' : 126\n",
      "'sound' : 128\n",
      "'uh' : 132\n",
      "'no' : 262\n",
      "'not' : 227\n",
      "'really' : 135\n",
      "'for' : 137\n",
      "'information' : 139\n",
      "'lot' : 142\n",
      "'more' : 143\n",
      "'ogres' : 270\n",
      "'than' : 146\n",
      "'people' : 147\n",
      "'think' : 297\n",
      "'example' : 150\n",
      "'are' : 271\n",
      "'like' : 272\n",
      "'onions' : 273\n",
      "'they' : 250\n",
      "'stink' : 157\n",
      "'yes' : 158\n",
      "'or' : 307\n",
      "'cry' : 164\n",
      "'leave' : 168\n",
      "'them' : 169\n",
      "'out' : 170\n",
      "'sun' : 173\n",
      "'all' : 176\n",
      "'brown' : 177\n",
      "'start' : 320\n",
      "'sprouting' : 179\n",
      "'little' : 180\n",
      "'white' : 181\n",
      "'hairs' : 182\n",
      "'layers' : 218\n",
      "'we' : 197\n",
      "'both' : 203\n",
      "'everybody' : 234\n",
      "'likes' : 235\n",
      "'cake' : 212\n",
      "'loves' : 214\n",
      "'cakes' : 229\n",
      "'care' : 221\n",
      "'everyone' : 223\n",
      "'else' : 233\n",
      "'parfait' : 317\n",
      "'ever' : 239\n",
      "'met' : 240\n",
      "'person' : 242\n",
      "'say' : 251\n",
      "'hey' : 252\n",
      "'let's' : 246\n",
      "'I' : 254\n",
      "'parfaits' : 282\n",
      "'delicious' : 287\n",
      "'dense' : 264\n",
      "'irritating' : 265\n",
      "'miniature' : 266\n",
      "'beast' : 267\n",
      "'burden' : 269\n",
      "'end' : 274\n",
      "'story' : 276\n",
      "'bye' : 278\n",
      "'see' : 279\n",
      "'ya' : 280\n",
      "'later' : 281\n",
      "'may' : 283\n",
      "'be' : 284\n",
      "'most' : 286\n",
      "'thing' : 288\n",
      "'damn' : 292\n",
      "'planet' : 293\n",
      "'preferred' : 299\n",
      "'humming' : 301\n",
      "'do' : 302\n",
      "'tissue' : 306\n",
      "'something' : 308\n",
      "'I'm' : 310\n",
      "'making' : 311\n",
      "'mess' : 313\n",
      "'word' : 316\n",
      "'makes' : 318\n",
      "'slobbering' : 321\n"
     ]
    }
   ],
   "source": [
    "for word, number in wordToNum.items():\n",
    "    print(f\"'{word}' : {number}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-straight",
   "metadata": {},
   "source": [
    "## download glove word embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-cathedral",
   "metadata": {},
   "source": [
    "- only download if glove embeddings are not currently in the workind directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "falling-integration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-09-12 10:22:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2023-09-12 10:22:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2023-09-12 10:22:04--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "100%[======================================>] 862,182,613 5.01MB/s   in 2m 39s \n",
      "\n",
      "2023-09-12 10:24:43 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n"
     ]
    }
   ],
   "source": [
    "# download glove and unzip it in Notebook.\n",
    "#!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "#!unzip glove*.zip\n",
    "  \n",
    "# vocab: 'the': 1, mapping of words with\n",
    "# integers in seq. 1,2,3..\n",
    "# embedding: 1->dense vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-poultry",
   "metadata": {},
   "source": [
    "# Get glove embeddings for each word in dic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-solid",
   "metadata": {},
   "source": [
    "- modify so that we return a dictionary of word : embedding instead of a matrix of index : embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "passing-ethics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_for_vocab(filepath, wordToNum, embedding_dim):\n",
    "    \n",
    "    ## create a dictionary to store embeddings from shrek\n",
    "    shrek_embeddings = {} \n",
    "    \n",
    "    ## for each word in the dictionary, grab the associated embedding in the glove file\n",
    "    with open(filepath, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in wordToNum:\n",
    "                embedding_vector = vector\n",
    "                shrek_embeddings[word] = embedding_vector\n",
    "                \n",
    "    return shrek_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "labeled-hopkins",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense vector for first word is =>  ['0.04656', '0.21318', '-0.0074364', '-0.45854', '-0.035639', '0.23643', '-0.28836', '0.21521', '-0.13486', '-1.6413', '-0.26091', '0.032434', '0.056621', '-0.043296', '-0.021672', '0.22476', '-0.075129', '-0.067018', '-0.14247', '0.038825', '-0.18951', '0.29977', '0.39305', '0.17887', '-0.17343', '-0.21178', '0.23617', '-0.063681', '-0.42318', '-0.11661', '0.093754', '0.17296', '-0.33073', '0.49112', '-0.68995', '-0.092462', '0.24742', '-0.17991', '0.097908', '0.083118', '0.15299', '-0.27276', '-0.038934', '0.54453', '0.53737', '0.29105', '-0.0073514', '0.04788', '-0.4076', '-0.026759', '0.17919', '0.010977', '-0.10963', '-0.26395', '0.07399', '0.26236', '-0.1508', '0.34623', '0.25758', '0.11971', '-0.037135', '-0.071593', '0.43898', '-0.040764', '0.016425', '-0.4464', '0.17197', '0.046246', '0.058639', '0.041499', '0.53948', '0.52495', '0.11361', '-0.048315', '-0.36385', '0.18704', '0.092761', '-0.11129', '-0.42085', '0.13992', '-0.39338', '-0.067945', '0.12188', '0.16707', '0.075169', '-0.015529', '-0.19499', '0.19638', '0.053194', '0.2517', '-0.34845', '-0.10638', '-0.34692', '-0.19024', '-0.2004', '0.12154', '-0.29208', '0.023353', '-0.11618', '-0.35768', '0.062304', '0.35884', '0.02906', '0.0073005', '0.0049482', '-0.15048', '-0.12313', '0.19337', '0.12173', '0.44503', '0.25147', '0.10781', '-0.17716', '0.038691', '0.08153', '0.14667', '0.063666', '0.061332', '-0.075569', '-0.37724', '0.01585', '-0.30342', '0.28374', '-0.042013', '-0.040715', '-0.15269', '0.07498', '0.15577', '0.10433', '0.31393', '0.19309', '0.19429', '0.15185', '-0.10192', '-0.018785', '0.20791', '0.13366', '0.19038', '-0.25558', '0.304', '-0.01896', '0.20147', '-0.4211', '-0.0075156', '-0.27977', '-0.19314', '0.046204', '0.19971', '-0.30207', '0.25735', '0.68107', '-0.19409', '0.23984', '0.22493', '0.65224', '-0.13561', '-0.17383', '-0.048209', '-0.1186', '0.0021588', '-0.019525', '0.11948', '0.19346', '-0.4082', '-0.082966', '0.16626', '-0.10601', '0.35861', '0.16922', '0.07259', '-0.24803', '-0.10024', '-0.52491', '-0.17745', '-0.36647', '0.2618', '-0.012077', '0.08319', '-0.21528', '0.41045', '0.29136', '0.30869', '0.078864', '0.32207', '-0.041023', '-0.1097', '-0.092041', '-0.12339', '-0.16416', '0.35382', '-0.082774', '0.33171', '-0.24738', '-0.048928', '0.15746', '0.18988', '-0.026642', '0.063315', '-0.010673', '0.34089', '1.4106', '0.13417', '0.28191', '-0.2594', '0.055267', '-0.052425', '-0.25789', '0.019127', '-0.022084', '0.32113', '0.068818', '0.51207', '0.16478', '-0.20194', '0.29232', '0.098575', '0.013145', '-0.10652', '0.1351', '-0.045332', '0.20697', '-0.48425', '-0.44706', '0.0033305', '0.0029264', '-0.10975', '-0.23325', '0.22442', '-0.10503', '0.12339', '0.10978', '0.048994', '-0.25157', '0.40319', '0.35318', '0.18651', '-0.023622', '-0.12734', '0.11475', '0.27359', '-0.21866', '0.015794', '0.81754', '-0.023792', '-0.85469', '-0.16203', '0.18076', '0.028014', '-0.1434', '0.0013139', '-0.091735', '-0.089704', '0.11105', '-0.16703', '0.068377', '-0.087388', '-0.039789', '0.014184', '0.21187', '0.28579', '-0.28797', '-0.058996', '-0.032436', '-0.0047009', '-0.17052', '-0.034741', '-0.11489', '0.075093', '0.099526', '0.048183', '-0.073775', '-0.41817', '0.0041268', '0.44414', '-0.16062', '0.14294', '-2.2628', '-0.027347', '0.81311', '0.77417', '-0.25639', '-0.11576', '-0.11982', '-0.21363', '0.028429', '0.27261', '0.031026', '0.096782', '0.0067769', '0.14082', '-0.013064', '-0.29686', '-0.079913', '0.195', '0.031549', '0.28506', '-0.087461', '0.0090611', '-0.20989', '0.053913']\n"
     ]
    }
   ],
   "source": [
    "# matrix for vocab: word_index\n",
    "embedding_dim = 300\n",
    "all_embeddings = f'glove.6B.{embedding_dim}d.txt'\n",
    "\n",
    "\n",
    "embedding_matrix_vocab = embedding_for_vocab(all_embeddings, wordToNum, embedding_dim)\n",
    "  \n",
    "print(\"Dense vector for first word is => \",\n",
    "      embedding_matrix_vocab[\"the\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "forbidden-prize",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "filename = \"shrek_embeddings.json\"\n",
    "with open(filename, \"w\") as json_file:\n",
    "    json.dump(embedding_matrix_vocab, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-ethiopia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "general-pressure",
   "metadata": {},
   "source": [
    "## get words from csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "vocal-console",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = pd.read_csv('black_gentle.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "provincial-dominican",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>word</th>\n",
       "      <th>onset</th>\n",
       "      <th>offset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So</td>\n",
       "      <td>so</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I</td>\n",
       "      <td>i</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>was</td>\n",
       "      <td>was</td>\n",
       "      <td>1.96</td>\n",
       "      <td>2.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>2.30</td>\n",
       "      <td>2.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>junior</td>\n",
       "      <td>junior</td>\n",
       "      <td>2.46</td>\n",
       "      <td>3.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Word    word  onset  offset\n",
       "0      So      so   0.24    0.63\n",
       "1       I       i   0.68    1.26\n",
       "2     was     was   1.96    2.30\n",
       "3       a       a   2.30    2.45\n",
       "4  junior  junior   2.46    3.14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vertical-devil",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['so', 'i', 'was', ..., 'do', 'thank', 'you'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = np.array(csv_file['word'])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foreign-psychology",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
