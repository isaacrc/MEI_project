{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "elder-rates",
   "metadata": {},
   "source": [
    "# `comp-03`: Voxelwise encoding models\n",
    "This demo introduces forward encoding modelsâ€”that is, using regularized regression to predict voxelwise activity from an explicit model of the stimulus in left-out data. Here, we use a semantic forward encoding model capturing the meaning of words to predict fMRI activity while participants listen to a spoken narrative (similarly to e.g. [Huth et al., 2016](https://doi.org/10.1038/nature17637)). The words in each TR are assigned a vector (i.e. word embedding) representing their location in a high-dimensional semantic space. For each voxel, we'll fit a model that predicts the time series of brain activity from the semantic vectors assigned to eachtime point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "metric-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "straight-athens",
   "metadata": {},
   "source": [
    "### Naturalistic story-listening dataset\n",
    "As an example, we'll use fMRI data collected for a single subject listening to a spoken story called \"[I Knew You Were Black](https://themoth.org/stories/i-knew-you-were-black)\" by Carol Daniel. These data are available as part of the publicly available [Narratives](https://github.com/snastase/narratives) collection ([Nastase et al., 2019](https://openneuro.org/datasets/ds002345)). Here, we'll download a single subject from the server for analysis. If you're working on the server, use `cp` on the command line to create a copy of the following file in your working directory; if you're working locally, use `scp` to download the file to your machine:\n",
    "\n",
    "`/jukebox/PNI-classes/students/NEU502/2023-NEU502B/brainiak-aperture-data/sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz`\n",
    "\n",
    "This dataset has been preprocessed using fMRIPrep with confound regression in AFNI. The functional data have been spatially normalized to a template in MNI space. To reduce computational demands, we compute parcel-wise ISCs using a cortical parcellation containing 400 parcels from Schaefer and colleages (2018). Load in the functional data and atlas. Use the parcellation to extract the mean functional time series for each parcel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "endless-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiMasker , MultiNiftiMasker, NiftiLabelsMasker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "engaging-salem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': b\"Yeo 2011 Atlas\\n\\n\\nNotes\\n-----\\nThis atlas provides a labeling of some cortical voxels in the MNI152\\nspace.\\n\\nFour versions of the atlas are available, according to the cortical\\nmodel (thick or thin cortical surface) and to the number of regions\\nconsidered (7 or 17).\\n\\nContent\\n-------\\n    :'anat': Background anatomical image for reference and visualization\\n    :'thin_7': Cortical parcelation into 7 regions, thin cortical model\\n    :'thin_17': Cortical parcelation into 17 regions, thin cortical model\\n    :'thick_7': Cortical parcelation into 17 regions, thick cortical model\\n    :'thick_17': Cortical parcelation into 17 regions, thick cortical model\\n    :'colors_7': Text file for the coloring of 7-regions parcellation\\n    :'colors_17': Text file for the coloring of 17-regions parcellation\\n\\n\\nReferences\\n----------\\nFor more information on this dataset's structure, see\\nhttp://surfer.nmr.mgh.harvard.edu/fswiki/CorticalParcellation_Yeo2011\\n\\nYeo BT, Krienen FM, Sepulcre J, Sabuncu MR, Lashkari D, Hollinshead M,\\nRoffman JL, Smoller JW, Zollei L., Polimeni JR, Fischl B, Liu H,\\nBuckner RL. The organization of the human cerebral cortex estimated by\\nintrinsic functional connectivity. J Neurophysiol 106(3):1125-65, 2011.\\n\\nLicence: unknown.\\n\",\n",
       " 'thin_7': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_7Networks_MNI152_FreeSurferConformed1mm.nii.gz',\n",
       " 'thick_7': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_7Networks_MNI152_FreeSurferConformed1mm_LiberalMask.nii.gz',\n",
       " 'thin_17': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_17Networks_MNI152_FreeSurferConformed1mm.nii.gz',\n",
       " 'thick_17': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_17Networks_MNI152_FreeSurferConformed1mm_LiberalMask.nii.gz',\n",
       " 'colors_7': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_7Networks_ColorLUT.txt',\n",
       " 'colors_17': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/Yeo2011_17Networks_ColorLUT.txt',\n",
       " 'anat': '/usr/people/isaacrc/nilearn_data/yeo_2011/Yeo_JNeurophysiol11_MNI152/FSL_MNI152_FreeSurferConformed_1mm.nii.gz'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "revised-solid",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "dataset = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\n",
    "atlas_filename = dataset.maps\n",
    "labels = dataset.labels\n",
    "\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True,\n",
    "                           memory='nilearn_cache', verbose=5)\n",
    "\n",
    "time_series = masker.fit_transform(func_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "significant-overall",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/people/isaacrc/nilearn_data/fsl/data/atlases/HarvardOxford/HarvardOxford-cort-maxprob-thr25-2mm.nii.gz'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "optional-musical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 48)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "isolated-amber",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "marine-chicken",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parc_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b156b8f0d86d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mroi_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matlas_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mroi_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matlas_img\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m196\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mroi_nii\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNifti1Image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparc_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matlas_nii\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplot_stat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroi_nii\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'viridis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'parc_img' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(7,5), tight_layout=True)\n",
    "axes[0].plot(time_series.T[:, 30])\n",
    "roi_img = np.zeros_like(atlas_img)\n",
    "roi_img[atlas_img==196] = 1\n",
    "roi_nii = nib.Nifti1Image(parc_img, atlas_nii.affine)\n",
    "plot_stat_map(roi_nii, axes=axes[1], vmax=1, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "changing-payday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftiLabelsMasker.fit_transform] loading data from /usr/people/isaacrc/nilearn_data/fsl/data/atlases/HarvardOxford/HarvardOxford-cort-maxprob-thr25-2mm.nii.gz\n",
      "Resampling labels\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling nilearn.input_data.base_masker.filter_and_extract...\n",
      "filter_and_extract('sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz', <nilearn.input_data.nifti_labels_masker._ExtractionFunctor object at 0x7fd4151b9fd0>, \n",
      "{ 'background_label': 0,\n",
      "  'detrend': False,\n",
      "  'dtype': None,\n",
      "  'high_pass': None,\n",
      "  'high_variance_confounds': False,\n",
      "  'labels_img': '/usr/people/isaacrc/nilearn_data/fsl/data/atlases/HarvardOxford/HarvardOxford-cort-maxprob-thr25-2mm.nii.gz',\n",
      "  'low_pass': None,\n",
      "  'mask_img': None,\n",
      "  'smoothing_fwhm': None,\n",
      "  'standardize': True,\n",
      "  'standardize_confounds': True,\n",
      "  'strategy': 'mean',\n",
      "  't_r': None,\n",
      "  'target_affine': None,\n",
      "  'target_shape': None}, confounds=None, dtype=None, memory=Memory(location=nilearn_cache/joblib), memory_level=1, verbose=5)\n",
      "[NiftiLabelsMasker.transform_single_imgs] Loading data from sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz\n",
      "[NiftiLabelsMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiLabelsMasker.transform_single_imgs] Cleaning extracted signals\n",
      "______________________________________________filter_and_extract - 15.3s, 0.3min\n"
     ]
    }
   ],
   "source": [
    "time_series = masker.fit_transform(func_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "compound-technical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 48)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "french-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_img = nib.load(func_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "built-flesh",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Data given cannot be loaded because it is not compatible with nibabel format:\n0.0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7d0525beff26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/input_data/multi_nifti_masker.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, imgs, y)\u001b[0m\n\u001b[1;32m    229\u001b[0m                               \u001b[0;34m' been provided at masker creation. Given mask'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                               ' will be used.' % self.__class__.__name__)\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_niimg_3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# If resampling is requested, resample the mask as well.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg_3d\u001b[0;34m(niimg, dtype)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    278\u001b[0m             return _iter_check_niimg(niimg, ensure_ndim=ensure_ndim,\n\u001b[1;32m    279\u001b[0m                                      dtype=dtype)\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_niimgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mconcat_niimgs\u001b[0;34m(niimgs, dtype, ensure_ndim, memory, memory_level, auto_resample, verbose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mfirst_niimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot concatenate empty objects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    278\u001b[0m             return _iter_check_niimg(niimg, ensure_ndim=ensure_ndim,\n\u001b[1;32m    279\u001b[0m                                      dtype=dtype)\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_niimgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mconcat_niimgs\u001b[0;34m(niimgs, dtype, ensure_ndim, memory, memory_level, auto_resample, verbose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mfirst_niimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot concatenate empty objects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    278\u001b[0m             return _iter_check_niimg(niimg, ensure_ndim=ensure_ndim,\n\u001b[1;32m    279\u001b[0m                                      dtype=dtype)\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_niimgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mconcat_niimgs\u001b[0;34m(niimgs, dtype, ensure_ndim, memory, memory_level, auto_resample, verbose)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mliterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtee\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         \u001b[0mfirst_niimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot concatenate empty objects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001b[0m in \u001b[0;36mcheck_niimg\u001b[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;31m# Otherwise, it should be a filename or a SpatialImage, we load it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0mniimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_niimg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_ndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mniimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/_utils/niimg.py\u001b[0m in \u001b[0;36mload_niimg\u001b[0;34m(niimg, dtype)\u001b[0m\n\u001b[1;32m    130\u001b[0m         raise TypeError(\"Data given cannot be loaded because it is\"\n\u001b[1;32m    131\u001b[0m                         \u001b[0;34m\" not compatible with nibabel format:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                         + _repr_niimgs(niimg, shorten=True))\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_target_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mniimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Data given cannot be loaded because it is not compatible with nibabel format:\n0.0"
     ]
    }
   ],
   "source": [
    "masker.fit()\n",
    "X_train = masker.transform(func_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-melbourne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-region",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "sweet-sensitivity",
   "metadata": {},
   "source": [
    "To orient ourselves a bit, let's try plotting the BOLD time series for an example parcel. Start with parcel `196` in left superior temporal auditory association cortex. After plotting the time series, plot the location of this parcel in the brain using `plot_stat_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chinese-nickname",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nilearn/datasets/__init__.py:90: FutureWarning: Fetchers from the nilearn.datasets module will be updated in version 0.9 to return python strings instead of bytes and Pandas dataframes instead of Numpy arrays.\n",
      "  \"Numpy arrays.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MultiNiftiMasker(detrend=True,\n",
       "                 mask_img=array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0....\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the time series for an example parcel:\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n",
    "masker\n",
    "# Plot parcel on MNI atlas:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "communist-sender",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filenames for intact notthefall data and Schaefer atlas\n",
    "func_fn = ('sub-284_task-black_space-MNI152NLin2009cAsym_res-native_desc-clean_bold.nii.gz')\n",
    "atlas_fn = ('Schaefer2018_400Parcels_17Networks_order_FSLMNI152_2.5mm.nii.gz')\n",
    "\n",
    "# Load in the Schaefer 400-parcel atlas\n",
    "atlas_nii = nib.load(atlas_fn)\n",
    "atlas_img = atlas_nii.get_fdata()\n",
    "\n",
    "# Load in intact functional data and compute parcel means:\n",
    "masker = MultiNiftiMasker(mask_img=atlas_img, detrend=True,\n",
    "                          standardize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "endangered-alabama",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot slice image objects; consider using `img.slicer[slice]` to generate a sliced image (see documentation for caveats) or slicing image array data with `img.dataobj[slice]` or `img.get_fdata()[slice]`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-e765943572fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfunc_parcels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_timepoints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_parcels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparcel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_parcels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfunc_parcels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparcel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_img\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0matlas_img\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mparcel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtight_layout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/jukebox/pkgs/PYGER/base/envs/0.11.0/lib/python3.7/site-packages/nibabel/spatialimages.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    585\u001b[0m         \"\"\"\n\u001b[1;32m    586\u001b[0m         raise TypeError(\n\u001b[0;32m--> 587\u001b[0;31m             \u001b[0;34m\"Cannot slice image objects; consider using `img.slicer[slice]` \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m             \u001b[0;34m\"to generate a sliced image (see documentation for caveats) or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[0;34m\"slicing image array data with `img.dataobj[slice]` or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot slice image objects; consider using `img.slicer[slice]` to generate a sliced image (see documentation for caveats) or slicing image array data with `img.dataobj[slice]` or `img.get_fdata()[slice]`"
     ]
    }
   ],
   "source": [
    "num_timepoints = func_img.shape[-1]\n",
    "num_parcels = 400\n",
    "func_parcels = np.zeros((int(num_timepoints), int(num_parcels)))\n",
    "for parcel in range(1, int(num_parcels) + 1):\n",
    "    func_parcels[:, parcel - 1] = func_img[atlas_img == parcel].mean(axis=0)\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(7,5), tight_layout=True)\n",
    "axes[0].plot(func_parcels[:, 196])\n",
    "roi_img = np.zeros_like(atlas_img)\n",
    "roi_img[atlas_img==196] = 1\n",
    "roi_nii = nib.Nifti1Image(parc_img, atlas_nii.affine)\n",
    "plot_stat_map(roi_nii, axes=axes[1], vmax=1, cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "universal-electricity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(550, 400)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_parcels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sharing-sally",
   "metadata": {},
   "source": [
    "### Semantic encoding model\n",
    "We'll decompose the story stimulus into a series of semantic features based on a simple, widely used representation of word meaning called GloVe ([Pennington et al., 2014](http://dx.doi.org/10.3115/v1/D14-1162])). GloVe represents each word in a 300-dimensional semantic space where words that are nearer in space are more semantically similar based on co-occurrence statistics from a large corpus of text. We start with a time-stamped stimulus transcript. For each TR, we identify which words occurred in that TR, then assign that TR the semantic embeddings associated with each word. For TRs with multiple words, we average the semantic embeddings; for TRs with no words, we use a 300-dimensional vector of zeros; we exclude a small number of stop words. Finally, to account for the hemodynamic lag in fMRI, we create lagged versions of the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "smooth-location",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in time-stamped transcript\n",
    "transcript_fn = 'black_gentle.csv'\n",
    "transcript = pd.read_csv(transcript_fn, sep=',')\n",
    "\n",
    "# Stimulus is roughly 800 seconds long\n",
    "tr = 1.5\n",
    "stim_dur = 800\n",
    "stim_trs = 800 // tr\n",
    "\n",
    "# Convert transcript to list for simplicity\n",
    "transcript = transcript.values.tolist()\n",
    "\n",
    "# Loop through TRs\n",
    "transcript_trs = []\n",
    "for t in np.arange(stim_trs):\n",
    "    \n",
    "    # Container for words in this TR\n",
    "    tr_words = []\n",
    "    \n",
    "    # Check if upcoming word onset is in this TR\n",
    "    while t * tr < transcript[0][2] <= t * tr + tr:\n",
    "        \n",
    "        # If so, pop this word out of list and keep it\n",
    "        w = transcript.pop(0)\n",
    "        tr_words.append(w[0])\n",
    "        \n",
    "    # Append words and move to the next TR\n",
    "    transcript_trs.append(tr_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "environmental-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in dictionary of GloVe embeddings\n",
    "with open('glove_embeddings.json') as f:\n",
    "    glove = json.load(f)\n",
    "    \n",
    "# Load list of standard stop words\n",
    "stopwords = np.load('nltk_stopwords.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "wireless-rochester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(transcript_trs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "brilliant-platform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['So', 'I'],\n",
       " ['was', 'a', 'junior'],\n",
       " ['in', 'college'],\n",
       " ['when', 'I', 'got', 'my', 'first'],\n",
       " ['paying'],\n",
       " ['in', 'my'],\n",
       " ['field', 'on', 'the'],\n",
       " ['radio', 'This', 'is'],\n",
       " ['not', 'an', 'internship'],\n",
       " [\"I'm\", 'getting', 'a', 'check'],\n",
       " [],\n",
       " ['It', 'was', 'a', 'country', 'and', 'western', 'radio'],\n",
       " ['station'],\n",
       " ['and', 'my'],\n",
       " ['job', 'though', 'it', 'was', 'only'],\n",
       " ['on', 'the', 'weekends'],\n",
       " ['was', 'to', 'play'],\n",
       " ['the', 'top', 'country'],\n",
       " ['hits', 'of'],\n",
       " ['the', 'week', 'each'],\n",
       " ['Sunday', 'It'],\n",
       " ['actually', 'came', 'on', 'an', 'album'],\n",
       " ['pre', 'recorded'],\n",
       " ['so', 'I'],\n",
       " ['had', 'to', 'take', 'it', 'out', 'of', 'the', 'sleeve'],\n",
       " [],\n",
       " ['and', 'put', 'it', 'on'],\n",
       " ['the', 'turntable', 'just', 'so'],\n",
       " ['and', 'put', 'the', 'needle'],\n",
       " ['down', 'on', 'side'],\n",
       " ['one', 'caring'],\n",
       " ['not', 'to', 'scratch'],\n",
       " ['it'],\n",
       " ['and', 'then', 'let', 'part', 'one', 'play'],\n",
       " ['My', 'moment'],\n",
       " ['is', 'coming', 'now'],\n",
       " ['Yes', 'all', 'the', 'training'],\n",
       " [\"it's\", 'coming', 'now', 'When'],\n",
       " ['side', 'one', 'would', 'end'],\n",
       " ['I', 'had', 'to', 'lift'],\n",
       " ['the', 'needle'],\n",
       " [],\n",
       " ['and', \"here's\", 'my', 'moment'],\n",
       " ['I'],\n",
       " ['get', 'to', 'read', 'the', 'weather', 'live'],\n",
       " ['for', 'Jefferson', 'City', 'Missouri'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Then', 'I', 'played', 'commercials', 'and'],\n",
       " ['while', 'the', 'commercials', 'were'],\n",
       " ['playing', 'I', 'had', 'to', 'flip'],\n",
       " ['the', 'album'],\n",
       " ['put', 'it', 'back'],\n",
       " ['down', 'on', 'the', 'turntable'],\n",
       " ['put', 'the', 'needle', 'back', 'down'],\n",
       " ['now', 'on', 'side', 'two'],\n",
       " ['and'],\n",
       " ['let', 'part', 'two', 'of', 'the', 'top'],\n",
       " ['country', 'hits', 'of', 'the', 'week'],\n",
       " ['play', 'You'],\n",
       " ['know', 'when', \"you're\", 'getting', 'a', 'degree'],\n",
       " ['in', 'communications'],\n",
       " ['or', 'broadcasting'],\n",
       " ['they', 'teach', 'you'],\n",
       " ['all', 'kinds', 'of', 'stuff'],\n",
       " ['but', 'they', 'do', 'not'],\n",
       " ['teach', 'you'],\n",
       " ['how', 'to', 'flip', 'an', 'album'],\n",
       " [],\n",
       " ['in', 'record', 'time'],\n",
       " ['before', 'the', 'commercial'],\n",
       " ['break', 'ends'],\n",
       " ['My', 'career'],\n",
       " ['would', 'eventually', 'bring', 'me', 'here'],\n",
       " ['to', 'St'],\n",
       " ['Louis'],\n",
       " [\"It's\", 'the'],\n",
       " ['largest', 'market'],\n",
       " ['I', 'had', 'ever', 'been', 'in'],\n",
       " ['I', 'was', 'excited'],\n",
       " ['and'],\n",
       " ['I', 'was', 'nervous'],\n",
       " ['and', 'so'],\n",
       " ['I', 'am'],\n",
       " ['trying', 'to', 'get', 'to', 'know', 'the', 'community'],\n",
       " ['And', 'so', \"I'm\"],\n",
       " ['going', 'to', 'every', 'fundraiser'],\n",
       " ['and', 'every', 'event'],\n",
       " ['I', 'can', 'think', 'of'],\n",
       " ['and', 'after', 'one', 'of', 'those', 'events'],\n",
       " ['I', 'promise', 'you', 'it', 'was'],\n",
       " ['actually', 'here', 'at'],\n",
       " ['the', 'Sheldon', 'in', 'St'],\n",
       " ['Louis', 'Missouri'],\n",
       " ['a', 'young', 'man', 'approached', 'me'],\n",
       " ['outside', 'on', 'the', 'sidewalk'],\n",
       " ['He', 'was', 'black'],\n",
       " ['and', 'he'],\n",
       " ['said', 'to', 'me'],\n",
       " ['I', 'knew', 'you'],\n",
       " ['were', 'black'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['Now', 'keep', 'in', 'mind'],\n",
       " ['that'],\n",
       " ['I', 'had', 'heard'],\n",
       " ['pretty', 'much', 'my', 'whole', 'adult', 'life'],\n",
       " ['You', 'sound', 'white'],\n",
       " [\"I'd\", 'heard', 'that'],\n",
       " [\"I'd\", 'heard'],\n",
       " ['Oh', 'you', 'sound', 'white'],\n",
       " ['You', 'sound', 'like'],\n",
       " ['a', 'white', 'girl', \"I'd\", 'heard', 'that'],\n",
       " ['but', 'I', 'had', 'never'],\n",
       " ['ever', 'heard'],\n",
       " ['I', 'knew'],\n",
       " ['you', 'were', 'black'],\n",
       " [],\n",
       " ['So', 'he'],\n",
       " ['tells', 'me', 'about'],\n",
       " ['this', 'debate', 'apparently'],\n",
       " ['that', 'had', 'been', 'going', 'on', 'in', 'St'],\n",
       " ['Louis'],\n",
       " ['when'],\n",
       " ['I', 'first', 'started', 'here'],\n",
       " ['People'],\n",
       " ['told', 'him', 'Oh', 'no'],\n",
       " ['no', \"she's\", 'white', 'man', \"she's\", 'white'],\n",
       " ['she', 'sounds', 'white', \"she's\", 'white'],\n",
       " ['and', 'he'],\n",
       " ['convinced'],\n",
       " ['having', 'never', 'met', 'me'],\n",
       " ['that', 'I', 'was', 'black'],\n",
       " ['Well', 'as'],\n",
       " ['it', 'turns', 'out', 'he', 'was'],\n",
       " ['right'],\n",
       " [],\n",
       " ['I', 'I', 'am'],\n",
       " ['black'],\n",
       " ['But', 'this', 'whole', 'debate'],\n",
       " ['sort', 'of', 'messed'],\n",
       " ['with', 'my', 'head', 'a', 'little', 'bit'],\n",
       " ['I', 'thought', 'here'],\n",
       " ['I', 'am', 'major', 'market'],\n",
       " ['fantastic'],\n",
       " ['job', 'of', 'my', 'dreams'],\n",
       " ['and', 'people'],\n",
       " ['still', \"don't\", 'know', 'who', 'I'],\n",
       " ['am', 'all', 'of'],\n",
       " ['who', 'I', 'am'],\n",
       " ['So', 'I', 'hatch'],\n",
       " ['the', 'secret'],\n",
       " ['plan'],\n",
       " ['This', 'mission'],\n",
       " ['I', \"didn't\", 'tell', 'anybody', 'about'],\n",
       " ['it', 'I', 'am', 'going'],\n",
       " ['to', 'start', 'dropping', 'hints', 'on', 'the'],\n",
       " ['air'],\n",
       " [],\n",
       " ['so', 'people'],\n",
       " ['so', 'people', 'will', 'know'],\n",
       " ['who', 'I', 'am'],\n",
       " ['And', 'so', 'my', 'first'],\n",
       " ['hint', 'is', 'me'],\n",
       " ['discussing', 'an', 'article'],\n",
       " [\"I'd\", 'read', 'in', 'Essence'],\n",
       " ['magazine'],\n",
       " [],\n",
       " ['Anybody', 'Essence'],\n",
       " ['Yes', 'you', 'get', 'it'],\n",
       " ['Essence', 'Magazine', 'of', 'course'],\n",
       " ['targets', 'African', 'American'],\n",
       " ['women', 'and', 'so', 'I', 'think', 'surely'],\n",
       " ['now', 'they', 'know', 'They', 'know'],\n",
       " ['They', 'got', 'to', 'know', 'now'],\n",
       " ['Debate', 'put', 'to', 'a', 'rest'],\n",
       " [],\n",
       " ['So', 'I', 'am', 'now'],\n",
       " ['all', 'about', 'town'],\n",
       " ['and', 'people', 'are', 'beginning', 'to', 'recognize'],\n",
       " ['me', 'just', 'from', 'my', 'voice'],\n",
       " [\"I'm\"],\n",
       " ['at', 'the', 'grocery', 'store'],\n",
       " [\"I'm\", 'chasing', 'my', 'kids'],\n",
       " ['at', 'the', 'St', 'Louis', 'zoo', 'yelling'],\n",
       " ['at', 'them', 'people', 'are', 'recognizing'],\n",
       " ['my', 'voice'],\n",
       " [],\n",
       " [\"I'm\", 'gonna', 'drop', 'another', 'hint'],\n",
       " ['on', 'the', 'air', \"I'm\"],\n",
       " ['enjoying', 'the', 'secret', 'mission'],\n",
       " ['And', 'so'],\n",
       " ['I', 'described'],\n",
       " ['trying', 'to'],\n",
       " ['make', 'my', \"mother's\", 'collard'],\n",
       " ['greens'],\n",
       " [],\n",
       " [],\n",
       " ['Boom'],\n",
       " [],\n",
       " ['They', 'got', 'to', 'know'],\n",
       " ['They', 'got', 'to'],\n",
       " ['know', 'now'],\n",
       " ['that', \"I'm\", 'a', 'black', 'woman'],\n",
       " ['One'],\n",
       " ['day', 'at', 'one', 'of', 'these', 'many', 'events'],\n",
       " ['an', 'education', 'panel'],\n",
       " ['discussion', 'after', \"it's\", 'over'],\n",
       " [\"I'm\", 'in', 'a', 'small', 'group'],\n",
       " ['of', 'people', 'and', 'we', 'are', 'talking'],\n",
       " ['and'],\n",
       " ['an', 'older', 'white', 'gentleman', 'walks'],\n",
       " ['up', 'to', 'me'],\n",
       " ['and', 'he', 'says'],\n",
       " ['You', 'speak', 'well'],\n",
       " ['for', 'your', 'people'],\n",
       " [],\n",
       " [],\n",
       " ['I', 'could', 'feel'],\n",
       " ['what', 'you', 'just', 'said'],\n",
       " ['I', 'could'],\n",
       " ['feel', 'it', 'in', 'my', 'stomach'],\n",
       " ['rising'],\n",
       " ['to', 'my', 'heart'],\n",
       " ['to', 'my', 'throat'],\n",
       " ['and', 'I', 'said', 'to', 'him'],\n",
       " ['well'],\n",
       " ['well', 'what', 'people', 'are', 'you', 'referring', 'to'],\n",
       " ['Now'],\n",
       " ['mhm', 'I', 'knew', 'what', 'he'],\n",
       " ['meant'],\n",
       " ['We', 'all', 'know', 'what'],\n",
       " ['he', 'meant'],\n",
       " ['but', 'I', 'wanted', 'him'],\n",
       " ['to', 'say', 'it', 'out', 'of', 'his', 'own', 'mouth'],\n",
       " ['I', 'wanted', 'him', 'to', 'say', 'it'],\n",
       " ['and', 'he', 'said', 'it'],\n",
       " [],\n",
       " ['You', 'know', 'black', 'people'],\n",
       " [],\n",
       " ['So', 'now', \"it's\"],\n",
       " ['really', 'gurgling', 'It', 'is'],\n",
       " ['threatening', 'to', 'come', 'spewing'],\n",
       " ['out', 'of', 'my', 'mouth'],\n",
       " ['I', 'am', 'embarrassed'],\n",
       " ['because', 'people'],\n",
       " ['are', 'standing', 'around'],\n",
       " ['and', \"they're\", 'hearing'],\n",
       " ['this', 'and', \"I'm\"],\n",
       " ['angry', 'How'],\n",
       " ['dare', 'he', 'diminish', 'me'],\n",
       " [],\n",
       " ['And', \"I'm\", 'indignant'],\n",
       " ['even'],\n",
       " ['and', 'before', 'I', 'could', 'say', 'another'],\n",
       " ['word', 'he'],\n",
       " ['says', 'to', 'me'],\n",
       " ['Have', 'you', 'had', 'any', 'training'],\n",
       " [],\n",
       " ['She'],\n",
       " ['said'],\n",
       " [],\n",
       " [],\n",
       " ['Well', 'I', 'said'],\n",
       " ['Nope'],\n",
       " [],\n",
       " [],\n",
       " ['My', 'father', 'speaks', 'this'],\n",
       " ['way', 'my', 'mother'],\n",
       " ['speaks', 'this', 'way', 'and', \"they're\"],\n",
       " ['from', 'Mississippi'],\n",
       " ['My', 'siblings', 'all'],\n",
       " ['speak', 'this', 'way'],\n",
       " ['God', 'given', 'talent'],\n",
       " ['no', 'training', 'no'],\n",
       " ['classes'],\n",
       " ['none', 'of', 'that', 'just'],\n",
       " ['little', 'old', 'me'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['But', \"I'm\", 'still', 'feeling'],\n",
       " ['the', 'secret', 'mission'],\n",
       " ['so', 'I'],\n",
       " ['decide', 'to', 'engage'],\n",
       " ['in', 'a', 'drop', 'the', 'mic', 'moment'],\n",
       " [\"It's\", 'closer'],\n",
       " ['to', 'Thanksgiving', 'now', 'when', 'I'],\n",
       " ['drop', 'this', 'hint'],\n",
       " ['and', 'I', 'describe', 'again'],\n",
       " ['um', 'one', 'of', 'my', \"mother's\", 'recipes'],\n",
       " ['for'],\n",
       " ['cornbread', 'dressing'],\n",
       " ['and', 'I', 'took', 'care'],\n",
       " ['on', 'the', 'air', 'to', 'say', 'not'],\n",
       " ['bread', 'not', 'white', 'bread'],\n",
       " ['dressing', 'but'],\n",
       " ['cornbread', 'dressing', 'where'],\n",
       " ['you', 'make', 'the', 'cornbread'],\n",
       " ['the', 'night', 'before'],\n",
       " ['and', 'then', 'you', 'soak', 'it', 'Cornbread'],\n",
       " ['dressing', 'and', 'I', 'thought'],\n",
       " ['boom', 'mic', 'moment', 'they', 'got'],\n",
       " ['to', 'know', \"I'm\", 'through', 'I', 'can', 'go', 'home'],\n",
       " ['now'],\n",
       " ['They', 'know', \"I'm\", 'black', 'They', 'know', \"I'm\", 'black'],\n",
       " ['now'],\n",
       " ['And', 'so'],\n",
       " ['on', 'the', 'air'],\n",
       " ['one', 'day'],\n",
       " [],\n",
       " ['I', 'was', 'just', 'remembering'],\n",
       " ['this', 'conversation'],\n",
       " [\"I'd\", 'had', 'with', 'these', 'two'],\n",
       " ['women', 'These'],\n",
       " ['two', 'petite'],\n",
       " ['older', 'black', 'women'],\n",
       " ['Gray', 'hair'],\n",
       " ['quaffed'],\n",
       " [],\n",
       " ['One', 'had', 'a', 'single', 'strand'],\n",
       " ['of', 'pearls'],\n",
       " ['They', 'both', 'had', 'a', 'touch'],\n",
       " ['of', 'makeup'],\n",
       " ['and', 'one', 'of', 'them', 'took'],\n",
       " ['me', 'by', 'each', 'hand', 'and'],\n",
       " ['she', 'pulled', 'me', 'close'],\n",
       " ['and', 'the', 'other', 'leaned', 'in', 'so', 'she'],\n",
       " ['could', 'hear'],\n",
       " ['and', 'she', 'said', 'to', 'me'],\n",
       " ['We'],\n",
       " ['are', 'so'],\n",
       " ['proud'],\n",
       " ['of', 'you'],\n",
       " [],\n",
       " ['And', 'I', 'got', 'a'],\n",
       " ['lump', 'in', 'my', 'throat'],\n",
       " ['And', 'I', 'knew', 'what', 'they', 'meant'],\n",
       " [\"They're\", 'looking', 'at'],\n",
       " ['me', 'this', 'young'],\n",
       " ['talented'],\n",
       " ['black'],\n",
       " ['woman', 'making'],\n",
       " ['her', 'mark', 'on'],\n",
       " ['this', 'big', 'radio', 'station'],\n",
       " ['in', 'St', 'Louis'],\n",
       " ['But', \"I'm\"],\n",
       " ['looking', 'at', 'them'],\n",
       " [],\n",
       " ['They', 'have', 'seen', 'and', 'experienced'],\n",
       " ['more', 'than', 'I', 'will', 'ever'],\n",
       " ['see', 'or', 'experience', 'or', 'have'],\n",
       " ['to', 'see', 'or'],\n",
       " ['experience'],\n",
       " [\"I'm\", 'looking', 'at', 'history'],\n",
       " ['in', 'its', 'face'],\n",
       " ['and', 'I'],\n",
       " ['can', 'feel', 'it'],\n",
       " ['One', 'day'],\n",
       " [\"we're\", 'in', 'this', 'debate'],\n",
       " ['on', 'the', 'air'],\n",
       " ['We', 'were', 'talking'],\n",
       " ['about', 'a', 'it', 'was', 'a', 'controversy'],\n",
       " [],\n",
       " ['An', 'organization', 'one', 'of', 'many'],\n",
       " [],\n",
       " ['decided', 'to', 'walk', 'out', 'onto'],\n",
       " ['the', 'highway'],\n",
       " ['not', 'a', 'road'],\n",
       " ['but', 'the', 'interstate'],\n",
       " ['and'],\n",
       " ['stop', 'the', 'traffic', 'and'],\n",
       " ['shut', 'it', 'down'],\n",
       " ['They', 'were', 'protesting'],\n",
       " ['the', 'lack', 'of'],\n",
       " ['minority', 'jobs', 'in', 'construction'],\n",
       " ['And'],\n",
       " ['so', 'one', 'of', 'the', 'organizations'],\n",
       " ['that', 'was', 'involved'],\n",
       " ['was', 'one', 'I', 'knew'],\n",
       " ['of', 'from', 'the', 'time'],\n",
       " ['I', 'lived', 'in', 'Kansas', 'City', 'Missouri', 'but'],\n",
       " ['my', 'co', 'host', 'at', 'the', 'time'],\n",
       " ['had', 'never', 'heard', 'of', 'them'],\n",
       " ['and', 'so', 'he', 'was', 'truly'],\n",
       " ['dismissive'],\n",
       " ['and', 'I', 'was', 'truly'],\n",
       " ['frustrated'],\n",
       " [],\n",
       " ['And', \"I'm\", 'thinking'],\n",
       " ['this', 'is', 'a', 'movement'],\n",
       " ['moment'],\n",
       " ['this', 'is', 'a', 'moment', 'where'],\n",
       " ['you', 'either', 'educate'],\n",
       " [],\n",
       " ['or', 'chastise'],\n",
       " ['or', 'enlighten', 'I'],\n",
       " ['think', \"I'm\", 'gonna', 'do'],\n",
       " ['all', 'three'],\n",
       " [],\n",
       " ['But', 'I', 'am', 'also'],\n",
       " ['thinking'],\n",
       " ['and', 'I', 'am', 'also'],\n",
       " ['worried'],\n",
       " [\"I'm\", 'still', 'kind', 'of', 'new', 'here'],\n",
       " [],\n",
       " [\"I'm\", 'still', 'young'],\n",
       " [\"I'm\", \"I'm\", 'a'],\n",
       " ['woman', \"I'm\", 'black'],\n",
       " ['and', \"I'm\", 'pregnant'],\n",
       " [],\n",
       " ['and', \"I'm\", 'worried'],\n",
       " ['about', 'the', 'stereotype'],\n",
       " ['of', 'the', 'angry', 'black', 'woman'],\n",
       " ['and', 'and', 'yet', \"I'm\", 'also', 'worried'],\n",
       " ['that', 'I', \"don't\", 'speak'],\n",
       " ['up', 'enough', 'about', 'black', 'issues'],\n",
       " [\"I'm\", 'thinking', 'all', 'of'],\n",
       " ['this'],\n",
       " ['And', 'I', 'dive'],\n",
       " ['right', 'in'],\n",
       " ['and', 'I', 'said'],\n",
       " ['to', 'him'],\n",
       " ['You', 'know', \"they're\", 'in', 'the', 'yellow', 'pages'],\n",
       " [],\n",
       " ['You', 'can', 'look', 'them', 'up', 'right', 'now'],\n",
       " ['and', 'call', 'them', \"they've\"],\n",
       " ['been', 'around', 'for', '50', 'years'],\n",
       " [],\n",
       " ['And'],\n",
       " ['so', 'we', 'took', 'some', 'calls', 'and'],\n",
       " ['we', 'ended', 'that', 'conversation'],\n",
       " [],\n",
       " ['turned', 'the', 'mics', 'off'],\n",
       " ['and', 'left', 'the', 'studio'],\n",
       " [],\n",
       " ['Now', 'just', 'a'],\n",
       " ['few', 'weeks', 'ago'],\n",
       " ['a', 'gentleman'],\n",
       " ['came', 'to', 'our', 'house', 'for', 'a', 'service'],\n",
       " ['call', 'to', 'repair', 'the', 'windshield'],\n",
       " ['of', 'my'],\n",
       " ['car'],\n",
       " [\"It's\", 'right', 'in', 'there', 'in', 'the', 'driveway'],\n",
       " ['I', 'met', 'him', 'outside'],\n",
       " ['and', 'he', 'said'],\n",
       " ['to', 'me', 'You', 'know', 'I', 'have', 'a', 'friend', 'who'],\n",
       " ['lives', 'over', 'here'],\n",
       " ['and', 'he', 'told', 'me'],\n",
       " ['that', 'that', 'news', 'lady'],\n",
       " ['lives', 'over', 'here'],\n",
       " [],\n",
       " ['Do', 'you', 'know'],\n",
       " ['her'],\n",
       " [],\n",
       " ['And'],\n",
       " ['I', 'said'],\n",
       " ['pausing'],\n",
       " ['feeling', 'pride'],\n",
       " ['and', 'trepidation'],\n",
       " [],\n",
       " ['and', 'I', 'said'],\n",
       " ['Well', 'I'],\n",
       " ['I', 'am', 'that'],\n",
       " ['woman'],\n",
       " ['You', 'are'],\n",
       " [\"I'm\", \"I'm\", 'Carol', 'Daniel'],\n",
       " ['And'],\n",
       " ['and', 'there', 'it', 'is', 'again'],\n",
       " [],\n",
       " [\"He's\", 'looking', 'at', 'me'],\n",
       " [\"He's\", 'sizing'],\n",
       " ['me', 'up'],\n",
       " ['He', 'sized', 'me'],\n",
       " ['up', 'in'],\n",
       " ['my', 'driveway'],\n",
       " [],\n",
       " ['and', 'I', 'thought'],\n",
       " ['Did'],\n",
       " ['he', 'think', 'I', 'was', 'white'],\n",
       " ['Is'],\n",
       " ['he', 'surprised', 'now', 'to', 'find', 'out', 'that', \"I'm\", 'black'],\n",
       " [],\n",
       " [],\n",
       " ['And', 'I', 'thought'],\n",
       " ['the', 'debate'],\n",
       " ['apparently', 'the', 'debate'],\n",
       " ['is'],\n",
       " ['still', 'there', 'in'],\n",
       " ['some', 'form', 'or', 'fashion'],\n",
       " ['I', 'am', 'not'],\n",
       " ['at', 'all', 'sure', 'I'],\n",
       " ['can', 'ever', 'really', 'do', 'anything'],\n",
       " ['about', 'it'],\n",
       " [\"There's\", 'a'],\n",
       " ['lot', 'more', 'at', 'stake'],\n",
       " [\"There's\", 'a', 'lot', 'more'],\n",
       " ['to', 'juggle', 'in'],\n",
       " ['my', 'life'],\n",
       " [\"There's\", 'a', 'lot', 'more'],\n",
       " ['weight', 'in', 'my', 'life'],\n",
       " ['I', 'just', 'try'],\n",
       " ['to', 'not', 'let', 'it', 'weigh', 'me'],\n",
       " ['down'],\n",
       " [],\n",
       " ['But', 'this', 'I', 'know'],\n",
       " [],\n",
       " ['I', 'deserve', 'to'],\n",
       " ['be', 'here'],\n",
       " ['I', 'deserve'],\n",
       " ['this', 'job'],\n",
       " [],\n",
       " [],\n",
       " ['And', 'I', \"don't\", 'have', 'this'],\n",
       " ['job', 'because'],\n",
       " ['I', 'sound', 'white'],\n",
       " ['I', 'have'],\n",
       " ['this', 'job'],\n",
       " ['because', 'I', 'am', 'good'],\n",
       " ['at', 'what', 'I', 'do'],\n",
       " ['Thank'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_trs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "stupid-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign GloVe embeddings to each TR:\n",
    "n_trs = len(transcript_trs)\n",
    "n_glove = len(glove['50'])\n",
    "embs = np.zeros([n_trs, n_glove])\n",
    "for tr in range(n_trs):\n",
    "    tr_words = [word for word in transcript_trs[tr] if word not in stopwords]\n",
    "    if len(tr_words)==0:\n",
    "        continue\n",
    "    tr_embs = np.stack([glove[word] for word in tr_words])\n",
    "    embs[tr] = tr_embs.mean(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-saudi",
   "metadata": {},
   "source": [
    "Now that we have a transcript, a dictionary of word embeddings, and a list of stop words, we're ready to construct our predictor matrix. For each TR, obtain the words from `transcript_trs`. If a word is in our list of stop words, ignore it; otherwise, obtain the GloVe embedding for that word. If there are multiple words in a TR (excluding stop words), average their word embeddings to obtain a single embedding per TR. If a TR is empty (i.e. contains no words), set the embedding to a zero vector of the same dimensionality as the other embeddings. Lastly, visualize the resulting predictor matrix; z-score each column (i.e. GloVe dimension) for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graduate-landscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign GloVe embeddings to each TR:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "generic-flour",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(533, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize predictor matrix of GloVe embeddings:\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finished-creation",
   "metadata": {},
   "source": [
    "Now, we'll horizontally stack the predictor matrix at lags of 2, 3, 4, and 5 TRs (3, 4.5, 6, 7.5 seconds) to account for variability in hemodynamic lag. Here's one recipe to do this: For each lag, first create prepending zero vectors for each TR in the lag; next, create appending zero vectors corresponding to the difference between the longest lag and the current lag; then, concatenate the prepending and appending vectors at the beginning and end of the predictor matrix, respectively. Horizontally stack the lagged matrices, z-score each column in the lagged predictor matrix, and visualize the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "answering-council",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Create temporally delayed version of embeddings:\n",
    "lags = [2, 3, 4, 5]\n",
    "\n",
    "# Horizontally stack lagged embeddings:\n",
    "for lag in lags:\n",
    "    \n",
    "    end = 5-lag\n",
    "    print(end)\n",
    "\n",
    "# Z-score embeddings:\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Plot lagged predictor matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "rapid-intersection",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_padding = [np.zeros([lag, embs.shape[1]]) for lag in lags]\n",
    "post_padding = [np.zeros([5-lag, embs.shape[1]]) for lag in lags]\n",
    "padded = np.hstack([np.vstack([pre, embs, post]) for pre, post in zip(pre_padding, post_padding)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-effect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuing-chase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "infectious-hollywood",
   "metadata": {},
   "source": [
    "The fMRI data were collected with an 8-TR buffer prior to the beginning of the story stimulus and a 9-TR buffer after the end of the story stimulus. Trim off the starting buffer TRs, as well as the ending buffer TRs (excluding TRs for the lags). This should finalize our predictor matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "directed-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim fMRI data to match embeddings:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-johnston",
   "metadata": {},
   "source": [
    "### Ridge regression\n",
    "Next, we use ridge regression to predict the activity at each parcel from the semantic embeddings. Note that the model dimensionality ($300 * 4$ delays $= 1200$ dimensions) is much greater than the number of samples ($<550$ TRs). This means we'll need to impose strong regularization on the model. Use a split-half outer cross-validation scheme where we train the model on half of the story and test the model on the other half. To search for the best-performing regularization parameter, perform 5-fold inner cross-validation within each training set using `RidgeCV`; this will select the best parameter setting from the inner cross-validation fold within the training half to predict the test half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-certificate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-half outer cross-validation fold:\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Ridge regression with alpha grid and nested CV:\n",
    "from sklearn.linear_model import RidgeCV\n",
    "alphas = [0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "\n",
    "# Loop through outer split-half cross-validation:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skilled-origin",
   "metadata": {},
   "source": [
    "### Evaluating model performance\n",
    "To evaluate the predictions of our model, we use the model weights estimated from the training data to predict the brain activity from semantic embeddings for the test data. We then assess the similarity between the predicted brain activity and the actual brain activity for the test data. Keeping with conventions in the literature, we use Pearson correlation to assess the match between predicted and actual brain activity (although this is not considered a good scoring metric in machine learning and there are many other options; e.g. $R^2$). For each parcel, compute the Pearson correlation between the actual and predicted test time series. Plot the predicted and actual time series for the superior temporal parcel `196` (z-score the actual and predicted time series for plotting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-bacteria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlation between predicted and actual responses:\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "    \n",
    "# Plot predicted and actual response for example parcel:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cosmetic-bruce",
   "metadata": {},
   "source": [
    "Finally, we'll plot the prediction performance scores on the brain. Convert the parcel-level prediction scores back into voxel-level brain maps; that is, for every voxel in a given parcel, assign that voxel the parcel's prediction score. Visualize the resulting brain map using `plot_stat_map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-infrastructure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot correlations between actual and predicted responses:\n",
    "from nilearn.plotting import plot_stat_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-football",
   "metadata": {},
   "source": [
    "#### References\n",
    "\n",
    "* Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. *Nature*, *532*(7600), 453-458. https://doi.org/10.1038/nature17637\n",
    "\n",
    "* Nastase, S. A., Liu, Y.-F., Hillman, H., Zadbood, A., Hasenfratz, L., Keshavarzian, N., Chen, J., Honey, C. J., Yeshurun, Y., Regev, M., Nguyen, M., Chang, C. H. C., Baldassano, C., Lositsky, O., Simony, E., Chow, M. A., Leong, Y. C., Brooks, P. P., Micciche, E., Choe, G., Goldstein, A., Vanderwal, T., Halchenko, Y. O., Norman, K. A., & Hasson, U. (2020). Narratives: fMRI data for evaluating models of naturalistic language comprehension. *bioRxiv*. https://doi.org/10.1101/2020.12.23.424091\n",
    "\n",
    "* Pennington, J., Socher, R., & Manning, C. D. (2014, October). GloVe: Global Vectors for Word Representation. In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)* (pp. 1532-1543). https://www.aclweb.org/anthology/D14-1162\n",
    "\n",
    "* Schaefer, A., Kong, R., Gordon, E. M., Laumann, T. O., Zuo, X. N., Holmes, A. J., ... & Yeo, B. T. (2018). Local-global parcellation of the human cerebral cortex from intrinsic functional connectivity MRI. Cerebral cortex, 28(9), 3095-3114. https://doi.org/10.1093/cercor/bhx179"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
